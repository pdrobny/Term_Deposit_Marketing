{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Term Depost Marketing\n",
        "\n",
        "## Background\n",
        "#### We are a small startup is focusing mainly on providing machine learning solutions in the European banking market. We work on a variety of problems including fraud detection, sentiment classification and customer intention prediction and classification.\n",
        "\n",
        "#### We are interested in developing a robust machine learning system that leverages information coming from call center data.\n",
        "\n",
        "#### Ultimately, we are looking for ways to improve the success rate for calls made to customers for any product that our clients offer. Towards this goal we are working on designing an ever evolving machine learning product that offers high success outcomes while offering interpretability for our clients to make informed decisions.\n",
        "\n",
        "## Goals\n",
        "#### - Predict if a customer will subscribe to a term deposit.\n",
        "#### - Find out which customers are more likely to buy the investment product.\n",
        "#### - Determine which features make the customer buy.  \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ORvWR4o8Lj0I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ------------------------------------------------------------------------\n",
        "## Setup"
      ],
      "metadata": {
        "id": "sW8rDGUORGBv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#install libraries\n",
        "!pip install pycaret\n",
        "!pip install imbalanced-learn\n",
        "!pip install optuna lightgbm\n",
        "!pip install duckdb\n",
        "!pip install nbconvert --upgrade"
      ],
      "metadata": {
        "id": "h3ADrjWLWWfi",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mPNJu6uzP7xl"
      },
      "outputs": [],
      "source": [
        "# import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import gdown\n",
        "import warnings\n",
        "import logging\n",
        "import random\n",
        "import lightgbm as lgb\n",
        "import requests\n",
        "import sys\n",
        "import optuna\n",
        "import duckdb as dd\n",
        "import umap\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.combine import SMOTEENN, SMOTETomek\n",
        "from sklearn import preprocessing\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC, LinearSVC\n",
        "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, BaggingClassifier, VotingClassifier, StackingClassifier, GradientBoostingClassifier\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.metrics import accuracy_score,classification_report, confusion_matrix, make_scorer, recall_score, precision_score, f1_score\n",
        "from sklearn.feature_selection import RFE\n",
        "from pycaret.datasets import get_data\n",
        "from pycaret.classification import setup, compare_models, predict_model\n",
        "warnings.filterwarnings('ignore', category=UserWarning)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# code to suppress lightGBM when running Lazy Classifer\n",
        "class CustomLogger:\n",
        "    def __init__(self):\n",
        "        self.logger = logging.getLogger('lightgbm_custom')\n",
        "        self.logger.setLevel(logging.ERROR)\n",
        "\n",
        "    def info(self, message):\n",
        "        self.logger.info(message)\n",
        "\n",
        "    def warning(self, message):\n",
        "        pass# Suppress warnings by not doing anything pass\n",
        "\n",
        "    def error(self, message):\n",
        "        self.logger.error(message)\n",
        "# Register the custom logger\n",
        "lgb.register_logger(CustomLogger())"
      ],
      "metadata": {
        "id": "V-u7dorj2o6Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# connect to personal google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "3edgyiKfUUoX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define function to download file from google drive\n",
        "def download_file_from_google_drive(id, destination):\n",
        "    URL = \"https://docs.google.com/uc?export=download\"\n",
        "\n",
        "    session = requests.Session()\n",
        "\n",
        "    response = session.get(URL, params={'id': id}, stream=True)\n",
        "    token = get_confirm_token(response)\n",
        "\n",
        "    if token:\n",
        "        params = {'id': id, 'confirm': token}\n",
        "        response = session.get(URL, params=params, stream=True)\n",
        "\n",
        "    save_response_content(response, destination)\n",
        "\n",
        "def get_confirm_token(response):\n",
        "    for key, value in response.cookies.items():\n",
        "        if key.startswith('download_warning'):\n",
        "            return value\n",
        "    return None\n",
        "\n",
        "def save_response_content(response, destination):\n",
        "    CHUNK_SIZE = 32768\n",
        "\n",
        "    with open(destination, \"wb\") as f:\n",
        "        for chunk in response.iter_content(CHUNK_SIZE):\n",
        "            if chunk:  # filter out keep-alive new chunks\n",
        "                f.write(chunk)\n",
        "\n",
        "file_id = '1EW-XMnGfxn-qzGtGPa3v_C63Yqj2aGf7'\n",
        "destination = 'term-deposit-marketing-2020.csv'\n",
        "download_file_from_google_drive(file_id, destination)\n",
        "df=pd.read_csv(destination)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "Wsilu4QLMqMm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# setup duck db connection\n",
        "conn = dd.connect()"
      ],
      "metadata": {
        "id": "0vzdzZfcbZxM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exploratory Data Analysis"
      ],
      "metadata": {
        "id": "qWi37wzgN7NV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# check for missing values\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "7209kRi7NpCF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check data types\n",
        "df.dtypes"
      ],
      "metadata": {
        "id": "1p7vh0dNOPmW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.describe())\n",
        "print(df.describe(include=['object']))"
      ],
      "metadata": {
        "id": "YUrH2SX1OTgV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# filter df for subscribed\n",
        "subscribed_df = conn.execute('''\n",
        "    SELECT *\n",
        "    FROM df\n",
        "    WHERE y = 'yes'\n",
        "      ''').fetchdf()\n",
        "\n",
        "# get statistical summary for continuous features\n",
        "print('subscribed:')\n",
        "print(subscribed_df.describe())\n",
        "\n",
        "# filter df for unsubscribed\n",
        "not_subscribed_df = conn.execute('''\n",
        "    SELECT *\n",
        "    FROM df\n",
        "    WHERE y = 'no'\n",
        "      ''').fetchdf()\n",
        "\n",
        "# get statistical summary for continuous features\n",
        "print('not subscribed:')\n",
        "print(not_subscribed_df.describe())"
      ],
      "metadata": {
        "id": "teq6fGiKkL_k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  get statistical summary for categorical features\n",
        "print('subscribed:')\n",
        "print(subscribed_df.describe(include=object))\n",
        "print('not subscribed:')\n",
        "print(not_subscribed_df.describe(include=object))"
      ],
      "metadata": {
        "id": "AhjWHEo3kMLr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# list unique values and counts in categorical columns\n",
        "for column in ['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact']:\n",
        "    unique_values = df[column].value_counts()\n",
        "    print(f\"Value counts in column '{column}':\")\n",
        "    print(unique_values)\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "id": "skloOWsUOYUd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Phase 1:  Determine customers that are more like to buy the investment product."
      ],
      "metadata": {
        "id": "P6kqQmw6abRp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Wrangling"
      ],
      "metadata": {
        "id": "UmtBFDsePziu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# replace yes, no with 1 and 0\n",
        "df.replace(['no', 'yes'], [0,1], inplace=True)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "thMPutqcOehF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a month number column\n",
        "month_map = {'jan': 1, 'feb': 2, 'mar': 3, 'apr': 4, 'may': 5, 'jun': 6,\n",
        "             'jul': 7, 'aug': 8, 'sep': 9, 'oct': 10, 'nov': 11, 'dec': 12}\n",
        "\n",
        "# Apply the mapping to the 'month' column\n",
        "df['month_number'] = df['month'].map(month_map)\n",
        "# Remove month column\n",
        "df.drop('month', axis=1, inplace=True)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "uCbR4TPJP4f1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#split data between target 'Y' and variables 'X'\n",
        "X = df.drop('y',axis=1)\n",
        "Y = df['y']\n",
        "print(Y)\n",
        "print(X.head())"
      ],
      "metadata": {
        "id": "yPibL6wIQMvG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data out customer related data\n",
        "X_customer_cols = ['age', 'job', 'marital', 'education', 'default', 'balance', 'housing', 'loan']\n",
        "X_customer_raw = X[X_customer_cols]\n",
        "X_customer_raw.head()"
      ],
      "metadata": {
        "id": "IB627xInQX0l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Visualization"
      ],
      "metadata": {
        "id": "Sv5UR9JkRtRX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace this with your actual data and labels\n",
        "y_counts = df['y'].value_counts()\n",
        "labels = y_counts.index\n",
        "sizes = y_counts.values\n",
        "\n",
        "\n",
        "plt.figure(figsize=(8, 6))  # Adjust figure size as needed\n",
        "plt.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=90)  # Use autopct for percentage display\n",
        "plt.title('Pie Chart of Y')\n",
        "\n",
        "# Add a legend with total values\n",
        "total = sum(sizes)\n",
        "legend_labels = [f'{label}: Total = {size}' for label, size in zip(labels,sizes)]\n",
        "plt.legend(legend_labels, loc=\"best\")\n",
        "\n",
        "\n",
        "plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Dtook9sXTdpH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Pie chart shows highly imbalance data with ~ 93% unsubsribed\n",
        "\n"
      ],
      "metadata": {
        "id": "kAe6cVgRXgwX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Subscription status by categorical features"
      ],
      "metadata": {
        "id": "KdBy21bZRuB2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12, 20))\n",
        "for i, col in enumerate(['job', 'marital', 'education', 'default', 'housing', 'loan']):\n",
        "    plt.subplot(5, 2, i + 1)\n",
        "    sns.countplot(data=df, x=col, hue='y')\n",
        "    plt.title(f'Subscription status count by {col}')\n",
        "    plt.xlabel(col)\n",
        "    plt.ylabel('Count')\n",
        "    plt.xticks(rotation=80)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0g72SeXZR4BW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Subscription status by continous features"
      ],
      "metadata": {
        "id": "2FopGxLBSZ8O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12, 20))\n",
        "for i, col in enumerate(['age', 'balance']):\n",
        "    plt.subplot(5, 2, i + 1)\n",
        "    sns.boxplot(y=col, x='y', data=df, hue='y')\n",
        "    plt.xlabel('Subscribed')\n",
        "    plt.ylabel(col)\n",
        "    plt.xticks(rotation=80)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kMBuuVB2SUpl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Rebalancing and Model Selection"
      ],
      "metadata": {
        "id": "zxmHzDbjX0bY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_customer_raw"
      ],
      "metadata": {
        "id": "lExhcBVuiAeG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert categorical columns with one hot encoding\n",
        "X_customer = pd.get_dummies(data=X_customer_raw, columns=(['job', 'marital', 'education']), drop_first=True)\n",
        "\n",
        "# scaling of continous features\n",
        "continous_cols = ['age', 'balance']\n",
        "transform = preprocessing.StandardScaler()\n",
        "X_customer[continous_cols] = transform.fit_transform(X_customer[continous_cols])\n",
        "# convert binary values to float type\n",
        "X_customer = X_customer.astype(float)\n",
        "X_customer.head()"
      ],
      "metadata": {
        "id": "thHa670icZPZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert target values to float type\n",
        "\n",
        "Y = Y.astype(float)\n",
        "Y.head()"
      ],
      "metadata": {
        "id": "M-EoeAbJcrWw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# good seeds 4134\n",
        "# seed = random.randint(1000,9999)\n",
        "seed = 4134\n",
        "print(seed)"
      ],
      "metadata": {
        "id": "Rd-hPmmIis8R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting training and testing sets before rebalancing to preserve\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_customer,Y, test_size=0.2, random_state=seed)\n",
        "print(f'X_train shape:',X_train.shape)\n",
        "print(f'y_train shape:',y_train.shape)\n",
        "print(f'X_test shape:',X_test.shape)\n",
        "print(f'y_test shape:',y_test.shape)"
      ],
      "metadata": {
        "id": "SnCkxV-QYXOX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Method 1:  Random Undersampler"
      ],
      "metadata": {
        "id": "9eDRYbV5YEyv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rus = RandomUnderSampler(random_state=seed)\n",
        "X_rus, y_rus = rus.fit_resample(X_train, y_train)\n",
        "print(f'X_train resample:',X_rus.shape)\n",
        "print(f'y_train resample:',y_rus.shape)"
      ],
      "metadata": {
        "id": "2vZ1u-FAjIup"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace this with your actual data and labels\n",
        "y_rus_counts = y_rus.value_counts()\n",
        "labels = y_rus_counts.index\n",
        "sizes = y_rus_counts.values\n",
        "\n",
        "\n",
        "plt.figure(figsize=(8, 6))  # Adjust figure size as needed\n",
        "plt.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=90)  # Use autopct for percentage display\n",
        "plt.title('Pie Chart of Y after rebalancing')\n",
        "\n",
        "# Add a legend with total values\n",
        "total = sum(sizes)\n",
        "legend_labels = [f'{label}: Total = {size}' for label, size in zip(labels,sizes)]\n",
        "plt.legend(legend_labels, loc=\"best\")\n",
        "\n",
        "\n",
        "plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Lel4osrbj_dy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Comparison after rebalacing with RUS\n",
        "\n"
      ],
      "metadata": {
        "id": "2BJwr8r7ljDw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize PyCaret\n",
        "clf = setup(data = pd.concat([X_rus, pd.DataFrame(y_rus, columns=['y'])], axis=1), target = 'y', fold = 5, session_id=seed)\n",
        "\n",
        "# Compare models\n",
        "best_model = compare_models()\n",
        "\n",
        "# Evaluate the best model on the test set (optional)\n",
        "predict_model(best_model, data=pd.concat([X_test, pd.DataFrame(y_test, columns=['y'])], axis=1));"
      ],
      "metadata": {
        "id": "EbS4NFLPlzcg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Method 2:  SMOTE-ENN"
      ],
      "metadata": {
        "id": "N7HFU-X6m9Ao"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sme = SMOTEENN(random_state=seed)\n",
        "X_sme, y_sme = sme.fit_resample(X_train, y_train)\n",
        "print(f'X_train resample:',X_sme.shape)\n",
        "print(f'y_train resample:',y_sme.shape)"
      ],
      "metadata": {
        "id": "GVAhMKGRnMvi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace this with your actual data and labels\n",
        "y_sme_counts = y_sme.value_counts()\n",
        "labels = y_sme_counts.index\n",
        "sizes = y_sme_counts.values\n",
        "\n",
        "\n",
        "plt.figure(figsize=(8, 6))  # Adjust figure size as needed\n",
        "plt.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=90)  # Use autopct for percentage display\n",
        "plt.title('Pie Chart of Y after rebalancing')\n",
        "\n",
        "# Add a legend with total values\n",
        "total = sum(sizes)\n",
        "legend_labels = [f'{label}: Total = {size}' for label, size in zip(labels,sizes)]\n",
        "plt.legend(legend_labels, loc=\"best\")\n",
        "\n",
        "\n",
        "plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "X9XxSCQmnM2p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Comparison after rebalacing with SMOTE-ENN"
      ],
      "metadata": {
        "id": "ihdlSjW_oHdy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize PyCaret\n",
        "clf = setup(data = pd.concat([X_sme, pd.DataFrame(y_sme, columns=['y'])], axis=1), target = 'y', fold = 5, session_id=seed)\n",
        "\n",
        "# Compare models\n",
        "best_model = compare_models()\n",
        "\n",
        "# Evaluate the best model on the test set (optional)\n",
        "predict_model(best_model, data=pd.concat([X_test, pd.DataFrame(y_test, columns=['y'])], axis=1));"
      ],
      "metadata": {
        "id": "UFPk-5SsnM9R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Method 3:  SMOTE-Tomek"
      ],
      "metadata": {
        "id": "SCLPMJRjnHXY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "smt = SMOTETomek(random_state=seed)\n",
        "X_smt, y_smt = smt.fit_resample(X_train, y_train)\n",
        "print(f'X_train resample:',X_smt.shape)\n",
        "print(f'y_train resample:',y_smt.shape)"
      ],
      "metadata": {
        "id": "R3JpPK2tneGS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace this with your actual data and labels\n",
        "y_smt_counts = y_smt.value_counts()\n",
        "labels = y_smt_counts.index\n",
        "sizes = y_smt_counts.values\n",
        "\n",
        "\n",
        "plt.figure(figsize=(8, 6))  # Adjust figure size as needed\n",
        "plt.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=90)  # Use autopct for percentage display\n",
        "plt.title('Pie Chart of Y after rebalancing')\n",
        "\n",
        "# Add a legend with total values\n",
        "total = sum(sizes)\n",
        "legend_labels = [f'{label}: Total = {size}' for label, size in zip(labels,sizes)]\n",
        "plt.legend(legend_labels, loc=\"best\")\n",
        "\n",
        "\n",
        "plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "57WdA0SoneNS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Comparison after rebalacing with SMOTE-Tomek"
      ],
      "metadata": {
        "id": "spiy8mG-odaQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize PyCaret\n",
        "clf = setup(data = pd.concat([X_smt, pd.DataFrame(y_smt, columns=['y'])], axis=1), target = 'y', fold = 5, session_id=seed)\n",
        "\n",
        "# Compare models\n",
        "best_model = compare_models()\n",
        "\n",
        "# Evaluate the best model on the test set (optional)\n",
        "predict_model(best_model, data=pd.concat([X_test, pd.DataFrame(y_test, columns=['y'])], axis=1));"
      ],
      "metadata": {
        "id": "XmGjSqDBneRL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Best peforming rebalancing method and model combination\n",
        "####  The Extra Trees classifer using SMOTE-ENN rebalanced data had the highest pefromance with an F1-score of 0.1655."
      ],
      "metadata": {
        "id": "JEoZMyuwh_6R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Optimatization and Evaluation"
      ],
      "metadata": {
        "id": "Et7CduVsSwye"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define a confusion matrix plotter for visualizing classification report results\n",
        "def plot_confusion_matrix(y,y_predict):\n",
        "    \"this function plots the confusion matrix\"\n",
        "    from sklearn.metrics import confusion_matrix\n",
        "\n",
        "    cm = confusion_matrix(y, y_predict)\n",
        "    ax= plt.subplot()\n",
        "    sns.heatmap(cm, annot=True, ax = ax, fmt='d'); #annot=True to annotate cells\n",
        "    ax.set_xlabel('Predicted labels')\n",
        "    ax.set_ylabel('True labels')\n",
        "    ax.set_title('Confusion Matrix');\n",
        "    ax.xaxis.set_ticklabels(['not subsribed', 'subscribed']); ax.yaxis.set_ticklabels(['not subsribed', 'subscribed'])\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "GoDcNjed2ud5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extra Trees Classifier with Smote-ENN re-balanced data\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "x5gC2nj6C-9N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# optimizing Extra trees classfier with optuna.  Class weight set to balanced to adjust class weights.\n",
        "\n",
        "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
        "def objective(trial):\n",
        "    n_estimators = trial.suggest_int(\"n_estimators\", 50, 500)\n",
        "    max_depth = trial.suggest_int(\"max_depth\", 2, 32, log=True)\n",
        "    min_samples_split = trial.suggest_int(\"min_samples_split\", 2, 20)\n",
        "    min_samples_leaf = trial.suggest_int(\"min_samples_leaf\", 1, 20)\n",
        "    max_features = trial.suggest_categorical(\"max_features\", ['sqrt', 'log2'])\n",
        "\n",
        "    model = ExtraTreesClassifier(\n",
        "        class_weight='balanced',\n",
        "        n_estimators=n_estimators,\n",
        "        max_depth=max_depth,\n",
        "        min_samples_split=min_samples_split,\n",
        "        min_samples_leaf=min_samples_leaf,\n",
        "        max_features=max_features,\n",
        "        random_state=seed\n",
        "    )\n",
        "    model.fit(X_sme, y_sme)\n",
        "    y_pred = model.predict(X_test)\n",
        "    f1 = f1_score(y_test, y_pred, pos_label=1)\n",
        "    return f1\n",
        "\n",
        "sampler = optuna.samplers.TPESampler(seed=seed)\n",
        "study = optuna.create_study(sampler=sampler, direction=\"maximize\")\n",
        "study.optimize(objective, n_trials=100, timeout=600)\n",
        "\n",
        "print(\"Number of finished trials:\", len(study.trials))\n",
        "print(\"Best trial parameters:\", study.best_trial.params)\n",
        "print(\"Best trial value:\", study.best_trial.value)\n",
        "\n",
        "best_params_et = study.best_trial.params\n",
        "et_best = ExtraTreesClassifier(**best_params_et, class_weight='balanced', random_state=seed)\n",
        "et_best.fit(X_sme, y_sme)\n",
        "y_pred = et_best.predict(X_test)\n",
        "\n",
        "report = classification_report(y_test, y_pred, target_names=['Not Subscribed', 'Subscribed'])\n",
        "print(report)\n",
        "plot_confusion_matrix(y_test, y_pred)"
      ],
      "metadata": {
        "id": "n0X-uU8Gd9hA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Final Model Results\n",
        "#### After optimization recall of subscribers improved to 0.54 while maintaining precision of 0.11.   The model successfully captured 54% of subscribers by reaching out to 66% less customers.  If the customer quantity was maintained at 8000 calls focusing only on likely subscribers as predicted by the model, 910 subscribers could be reached compared to the current 578, a 57% increase."
      ],
      "metadata": {
        "id": "W87Z05Lxjg_R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Convert the confusion matrix to a DataFrame for easier handling\n",
        "cm_df = pd.DataFrame(cm, columns=['Predicted Not Subscribed', 'Predicted Subscribed'],\n",
        "                     index=['Actual Not Subscribed', 'Actual Subscribed'])\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "cm_df.to_csv('confusion_matrix.csv')\n",
        "\n"
      ],
      "metadata": {
        "id": "5c5XqPRg5EML"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Customer segmentation"
      ],
      "metadata": {
        "id": "PQZRrr6UdvOc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_test.head()"
      ],
      "metadata": {
        "id": "5bkl3kOqOOGA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting training and testing sets before rebalancing to preserve\n",
        "X_train_raw, X_test_raw, y_train_raw, y_test_raw = train_test_split(X,Y, test_size=0.2, random_state=seed)\n",
        "X_test_raw.head()"
      ],
      "metadata": {
        "id": "FQHqkMtxgYgF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# add y_pred to X using X index\n",
        "\n",
        "df_pred = X_test_raw\n",
        "\n",
        "df_pred['y_pred'] = y_pred\n",
        "df_pred['y_actual'] = y_test_raw\n",
        "print(df_pred.shape)\n",
        "print(df_pred.head())\n",
        "\n"
      ],
      "metadata": {
        "id": "Kk1rFUIWRMw5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sum call durations for predicted subscribers and total\n",
        "true_pos_sub = cm[1,1]\n",
        "actual_sub = df_pred['y_actual'].sum()\n",
        "\n",
        "print(f\"Predicted Subscribers: {true_pos_sub}\")\n",
        "print(f\"Actual Subscribers: {actual_sub}\")\n",
        "\n",
        "# sum call durations for predicted subscribers and total\n",
        "campaigns_pred_sub = df_pred.loc[df_pred['y_pred'] == 1, 'campaign'].sum()\n",
        "campaigns_total = df_pred['campaign'].sum()\n",
        "\n",
        "print(f\"Sum of campaigns where y_pred = 1: {campaigns_pred_sub}\")\n",
        "print(f\"Sum of campaigns for all y_pred: {campaigns_total}\")\n",
        "\n",
        "sub_rate_pred = campaigns_pred_sub/true_pos_sub\n",
        "sub_rate_actual = campaigns_total/actual_sub\n",
        "\n",
        "print(f\"Campaigns per subcriber predicted: {sub_rate_pred}\")\n",
        "print(f\"Campaigns per subscriber actual: {sub_rate_actual}\")\n",
        "\n",
        "sub_rate_improvement = (sub_rate_actual - sub_rate_pred)/sub_rate_actual\n",
        "print(f\"Improvement in campaigns per subscriber: {sub_rate_improvement}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "9U04Gro1AENY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# add y_pred to X expanded using X index\n",
        "df_pred_exp = []\n",
        "df_pred_exp = X_test\n",
        "df_pred_exp['campaign'] = df_pred['campaign']\n",
        "df_pred_exp['age'] = df_pred['age']\n",
        "df_pred_exp['balance'] = df_pred['balance']\n",
        "df_pred_exp['y_pred'] = y_pred\n",
        "df_pred_exp.head()"
      ],
      "metadata": {
        "id": "TgItRpz6-PKz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# filter df for subscribed\n",
        "predicted_sub = conn.execute('''\n",
        "    SELECT *\n",
        "    FROM df_pred_exp\n",
        "    WHERE y_pred = 1\n",
        "      ''').fetchdf()\n",
        "\n",
        "predicted_sub = predicted_sub.drop('y_pred', axis=1)\n",
        "predicted_sub.head()\n"
      ],
      "metadata": {
        "id": "SB2NW1CdfhIs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select relevant features for clustering\n",
        "features_for_clustering = ['age', 'balance']  # You can add more features if needed\n",
        "X_cluster = predicted_sub[features_for_clustering]\n",
        "\n",
        "# Scale the features\n",
        "scaler = StandardScaler()\n",
        "X_cluster_scaled = scaler.fit_transform(X_cluster)\n",
        "\n",
        "# Determine the optimal number of clusters using the elbow method (optional)\n",
        "n_samples = X_cluster_scaled.shape[0]\n",
        "inertia = []\n",
        "for i in range(1, min(11, n_samples)):  # Limit clusters to less than or equal to number of samples\n",
        "    kmeans = KMeans(n_clusters=i, random_state=seed)\n",
        "    kmeans.fit(X_cluster_scaled)\n",
        "    inertia.append(kmeans.inertia_)\n",
        "\n",
        "plt.plot(range(1, min(11, n_samples)), inertia, marker='o') #update range for plot\n",
        "plt.title('Elbow Method for Optimal K')\n",
        "plt.xlabel('Number of clusters')\n",
        "plt.ylabel('Inertia')\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "s4ECXNyZQmCZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Using the elbow method, the optimal number of clusters is 3."
      ],
      "metadata": {
        "id": "TSRq9GlEoW0j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Based on the elbow method or other analysis, choose the number of clusters\n",
        "n_clusters = 4  # Replace with the optimal number of clusters\n",
        "\n",
        "# Perform K-means clustering\n",
        "kmeans = KMeans(n_clusters=n_clusters, random_state=seed)\n",
        "kmeans.fit(X_cluster_scaled)\n",
        "\n",
        "# Add cluster labels to the DataFrame\n",
        "predicted_sub['cluster'] = kmeans.labels_\n",
        "\n",
        "# show count of each\n",
        "count = predicted_sub.groupby('cluster').size()\n",
        "print(count)\n",
        "# Select only numeric columns for calculating the mean\n",
        "numeric_predicted_sub = predicted_sub.select_dtypes(include=np.number)\n",
        "\n",
        "print(numeric_predicted_sub.groupby('cluster').mean()) # Calculate mean for numeric columns only\n",
        "\n",
        "# You can also visualize the clusters using scatter plots or other methods.\n",
        "# For example:\n",
        "#sns.scatterplot(x='age', y='balance', hue='cluster', data=predicted_sub)\n",
        "#plt.show()\n",
        "\n",
        "#print(predicted_sub.head())\n"
      ],
      "metadata": {
        "id": "nSjLe6SEUyfh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.countplot(data=predicted_sub, x='cluster', hue='cluster')\n",
        "plt.title('Count of Each Cluster')\n",
        "plt.xlabel('Cluster')\n",
        "plt.ylabel('Count')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "w2Rj7nqM2Lrw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Customer segments\n",
        "#### 4 customer segments were identified.  With 1 segment containing only 1 customer, it will be ignored for analysis.  \n",
        "\n",
        "#### Common between the 3 remaining cluster are majority have education secondary or above.  No personal loans or credit in default. On average have a positive balance.  It approx. 3 calls to turn the customers into subscribers.\n",
        "\n",
        "#### Young/Unmarried:  Average age of 32 yrs, 72% single, higher rate of having a home loan, with an average balance of ~$900.\n",
        "\n",
        "#### Older/Married:  Average age of 51 yrs, 64% married, lower rate of having a home loan, with an average balance of ~$1200.\n",
        "\n",
        "#### Middle aged/College Educated:  Middle aged with average age of 40 yrs, ~50% are in management, 67% college educated, with an average balance of $12000.\n"
      ],
      "metadata": {
        "id": "7yfDQG7ztaG0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cluster visualization using T-SNE"
      ],
      "metadata": {
        "id": "UlucOfHUsJXE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_values = predicted_sub.values\n",
        "# Perform t-SNE dimensionality reduction\n",
        "tsne = TSNE(n_components=2, random_state=seed)\n",
        "X_tsne = tsne.fit_transform(X_values)\n",
        "\n",
        "# Create a DataFrame with t-SNE coordinates and cluster labels\n",
        "tsne_df = pd.DataFrame({'x': X_tsne[:, 0], 'y': X_tsne[:, 1], 'cluster': predicted_sub['cluster']})\n",
        "\n",
        "# Plot the t-SNE visualization, colored by cluster\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.scatterplot(x='x', y='y', hue='cluster', data=tsne_df, palette='viridis')\n",
        "plt.title('t-SNE Visualization of Predicted Subscribers, Colored by Cluster')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ZG99NR8321N3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cluster visualization using UMAP"
      ],
      "metadata": {
        "id": "IIeHYwVSsntj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply UMAP\n",
        "reducer = umap.UMAP(n_components=2, random_state=seed)  # Reduce to 2D for visualization\n",
        "X_umap = reducer.fit_transform(X_values)\n",
        "\n",
        "# Create a DataFrame with UMAP coordinates and cluster labels\n",
        "umap_df = pd.DataFrame({'x': X_umap[:, 0], 'y': X_umap[:, 1], 'cluster': predicted_sub['cluster']})\n",
        "\n",
        "# Plot the UMAP visualization, colored by cluster\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.scatterplot(x='x', y='y', hue='cluster', data=umap_df, palette='viridis')\n",
        "plt.title('UMAP Visualization of Predicted Subscribers, Colored by Cluster')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fvi7XZIa5wYZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusions.\n",
        "\n",
        "#### A predictive model was developed improving the customer subscription rate from 7% to 11%, a 57% increase.  The rate of campaigns needed per subsriber improved from 40 to 26, a 35% reduction. Three distinct groups were identified as likely customers, young/unmarried, older/married, middle-aged/college educated.  Common between the groups was a high rate of education level secondary or above, positive balances, and no credit default or personal loans."
      ],
      "metadata": {
        "id": "r_wnD2IODzoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "7Dg_rVUMD1ta"
      }
    }
  ]
}
